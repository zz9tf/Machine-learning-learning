{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Machine-learning-learning\\adult_data.csv\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>gross-income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          workclass  fnlwgt    education  education-num  \\\n",
       "0       39          State-gov   77516    Bachelors             13   \n",
       "1       50   Self-emp-not-inc   83311    Bachelors             13   \n",
       "2       38            Private  215646      HS-grad              9   \n",
       "3       53            Private  234721         11th              7   \n",
       "4       28            Private  338409    Bachelors             13   \n",
       "...    ...                ...     ...          ...            ...   \n",
       "32556   27            Private  257302   Assoc-acdm             12   \n",
       "32557   40            Private  154374      HS-grad              9   \n",
       "32558   58            Private  151910      HS-grad              9   \n",
       "32559   22            Private  201490      HS-grad              9   \n",
       "32560   52       Self-emp-inc  287927      HS-grad              9   \n",
       "\n",
       "            marital-status          occupation    relationship    race  \\\n",
       "0            Never-married        Adm-clerical   Not-in-family   White   \n",
       "1       Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "2                 Divorced   Handlers-cleaners   Not-in-family   White   \n",
       "3       Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
       "4       Married-civ-spouse      Prof-specialty            Wife   Black   \n",
       "...                    ...                 ...             ...     ...   \n",
       "32556   Married-civ-spouse        Tech-support            Wife   White   \n",
       "32557   Married-civ-spouse   Machine-op-inspct         Husband   White   \n",
       "32558              Widowed        Adm-clerical       Unmarried   White   \n",
       "32559        Never-married        Adm-clerical       Own-child   White   \n",
       "32560   Married-civ-spouse     Exec-managerial            Wife   White   \n",
       "\n",
       "           sex  capital-gain  capital-loss  hours-per-week  native-country  \\\n",
       "0         Male          2174             0              40   United-States   \n",
       "1         Male             0             0              13   United-States   \n",
       "2         Male             0             0              40   United-States   \n",
       "3         Male             0             0              40   United-States   \n",
       "4       Female             0             0              40            Cuba   \n",
       "...        ...           ...           ...             ...             ...   \n",
       "32556   Female             0             0              38   United-States   \n",
       "32557     Male             0             0              40   United-States   \n",
       "32558   Female             0             0              40   United-States   \n",
       "32559     Male             0             0              20   United-States   \n",
       "32560   Female         15024             0              40   United-States   \n",
       "\n",
       "      gross-income  \n",
       "0            <=50K  \n",
       "1            <=50K  \n",
       "2            <=50K  \n",
       "3            <=50K  \n",
       "4            <=50K  \n",
       "...            ...  \n",
       "32556        <=50K  \n",
       "32557         >50K  \n",
       "32558        <=50K  \n",
       "32559        <=50K  \n",
       "32560         >50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def load_data(file_path='data/train.csv', df=None, drop_columns=None, target_column=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file\n",
    "    drop_columns : list\n",
    "        List of columns to drop\n",
    "    target_column : str\n",
    "        Name of the target column\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix\n",
    "    y : pandas.Series\n",
    "        Target variable\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    if df is None:\n",
    "        df = pd.read_csv(file_path)\n",
    "    if drop_columns:\n",
    "        # Drop specified columns\n",
    "        df.drop(columns=drop_columns, inplace=True)\n",
    "    if target_column is None:\n",
    "        return df\n",
    "    # Extract target variable\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Remove target from features\n",
    "    df.drop(columns=[target_column], inplace=True)\n",
    "    \n",
    "    # Set feature matrix\n",
    "    X = df\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "path = './adult_data.csv'\n",
    "print(os.path.abspath(path))\n",
    "print(os.path.exists(os.path.abspath(path)))\n",
    "# for filename in os.listdir(path):\n",
    "#     print(filename)\n",
    "\n",
    "df = pd.read_csv(path, skiprows=[0,1])\n",
    "df = load_data(df=df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Basic filtering\n",
    "# df[(df['column1'] == 'value1') | (df['column2'] == 'value2') & (df['column3'] == 'value3')]\n",
    "\n",
    "# select columns by index\n",
    "# columns =  df.columns # get columns\n",
    "# df[['age','hours-per-week']] # by name\n",
    "# df[columns[[1,5,7]]] # by columns\n",
    "# df.iloc[:,3] # a number\n",
    "# df.iloc[:,[3,5,6]] # a list\n",
    "# df.iloc[:,::2] # a range\n",
    "##########################\n",
    "# Join\n",
    "# we are interested in only patients from hospital1\n",
    "# df_left = df1.merge(df2,how='left',on='ID') # IDs from the left dataframe (df1) are kept\n",
    "# print(df_left)\n",
    "\n",
    "# we are interested in only patients from hospital2\n",
    "#df_right = df1.merge(df2,how='right',on='ID') # IDs from the right dataframe (df2) are kept\n",
    "#print(df_right)\n",
    "\n",
    "# we are interested in patiens who were in both hospitals\n",
    "#df_inner = df1.merge(df2,how='inner',on='ID') # merging on IDs present in both dataframes\n",
    "#print(df_inner)\n",
    "\n",
    "# we are interested in all patients who visited at least one of the hospitals\n",
    "#df_outer = df1.merge(df2,how='outer',on='ID')  # merging on IDs present in any dataframe\n",
    "#print(df_outer)\n",
    "############################\n",
    "# Append dataframe\n",
    "# df_append = pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_visualize_dataframe(df, figsize=(5, 3), continuous_bins='auto', \n",
    "                                   log_threshold=1000, max_categories=15, \n",
    "                                   plot_all=True, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the statistical distribution of each column in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame to analyze\n",
    "    figsize : tuple, optional (default=(5, 3))\n",
    "        Figure size for plots\n",
    "    continuous_bins : str or int, optional (default='auto')\n",
    "        Number of bins for continuous variables, 'auto' uses unique values or sqrt rule\n",
    "    log_threshold : int, optional (default=1000)\n",
    "        Threshold for using log scale if max/min ratio exceeds this value\n",
    "    max_categories : int, optional (default=15)\n",
    "        Maximum number of categories to display in bar plots\n",
    "    plot_all : bool, optional (default=True)\n",
    "        Whether to generate plots for all columns\n",
    "    exclude_cols : list, optional (default=None)\n",
    "        List of column names to exclude from analysis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with analysis results for each column\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    exclude_cols = exclude_cols or []\n",
    "    \n",
    "    # Track invisible/problematic data\n",
    "    invisible_data = {}\n",
    "    \n",
    "    # Function to determine if a column should use log scale\n",
    "    def should_use_log_scale(series):\n",
    "        if series.min() <= 0:\n",
    "            return False\n",
    "        if series.max() / max(series.min(), 1e-10) > log_threshold:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    print(f\"All columns and their data types ({len(df.columns)}):\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for col in df.columns:\n",
    "        if col in exclude_cols:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'='*50}\\nAnalyzing column: {col}\\n{'='*50}\")\n",
    "        \n",
    "        # Store basic information\n",
    "        results[col] = {\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'is_numeric': pd.api.types.is_numeric_dtype(df[col]),\n",
    "            'unique_count': df[col].nunique(),\n",
    "            'missing_count': df[col].isna().sum(),\n",
    "            'missing_percentage': (df[col].isna().sum() / len(df)) * 100\n",
    "        }\n",
    "        \n",
    "        # Print basic statistics\n",
    "        print(f\"Data type: {results[col]['dtype']}\")\n",
    "        print(f\"Unique values: {results[col]['unique_count']}\")\n",
    "        print(f\"Missing values: {results[col]['missing_count']} ({results[col]['missing_percentage']:.2f}%)\")\n",
    "        \n",
    "        # Track invisible data\n",
    "        if results[col]['missing_count'] > 0:\n",
    "            invisible_data[col] = {'missing': results[col]['missing_count']}\n",
    "            \n",
    "        # For numeric columns\n",
    "        if results[col]['is_numeric']:\n",
    "            # Calculate descriptive statistics\n",
    "            desc = df[col].describe()\n",
    "            results[col]['stats'] = desc.to_dict()\n",
    "            print(\"\\nDescriptive statistics:\")\n",
    "            print(desc)\n",
    "            \n",
    "            # Detect zeros and infinities\n",
    "            zero_count = (df[col] == 0).sum()\n",
    "            inf_count = np.isinf(df[col]).sum() if np.issubdtype(df[col].dtype, np.number) else 0\n",
    "            \n",
    "            if zero_count > 0:\n",
    "                print(f\"Zero values: {zero_count} ({zero_count/len(df)*100:.2f}%)\")\n",
    "                invisible_data.setdefault(col, {})['zeros'] = zero_count\n",
    "                \n",
    "            if inf_count > 0:\n",
    "                print(f\"Infinite values: {inf_count} ({inf_count/len(df)*100:.2f}%)\")\n",
    "                invisible_data.setdefault(col, {})['infinities'] = inf_count\n",
    "            \n",
    "            # Detect if it should use log scale\n",
    "            use_log = should_use_log_scale(df[col].dropna()) if log_threshold else False\n",
    "            if use_log:\n",
    "                print(\"Distribution is highly skewed, log scale recommended\")\n",
    "                results[col]['log_recommended'] = True\n",
    "            else:\n",
    "                results[col]['log_recommended'] = False\n",
    "                \n",
    "            # Plot histogram if requested\n",
    "            if plot_all:\n",
    "                plt.figure(figsize=figsize)\n",
    "                \n",
    "                # Determine bin count\n",
    "                if continuous_bins == 'auto':\n",
    "                    if results[col]['unique_count'] < 50:\n",
    "                        bins = results[col]['unique_count']\n",
    "                    else:\n",
    "                        bins = int(np.sqrt(df.shape[0]))  # Square root rule\n",
    "                else:\n",
    "                    bins = continuous_bins\n",
    "                \n",
    "                # Use log scale if appropriate\n",
    "                if use_log and df[col].min() > 0:\n",
    "                    non_zero = df[col][df[col] > 0]\n",
    "                    log_bins = np.logspace(\n",
    "                        np.log10(max(non_zero.min(), 1e-10)),\n",
    "                        np.log10(non_zero.max()),\n",
    "                        bins\n",
    "                    )\n",
    "                    non_zero.plot.hist(bins=log_bins, log=True)\n",
    "                    plt.semilogx()\n",
    "                    print(\"Using log scale for visualization\")\n",
    "                else:\n",
    "                    df[col].plot.hist(bins=bins)\n",
    "                    \n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel('count')\n",
    "                plt.title(f'Distribution of {col}')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "        # For categorical columns\n",
    "        else:\n",
    "            # Get value counts\n",
    "            value_counts = df[col].value_counts()\n",
    "            top_n = min(max_categories, len(value_counts))\n",
    "            \n",
    "            print(f\"\\nTop {top_n} values:\")\n",
    "            print(value_counts.head(top_n))\n",
    "            \n",
    "            if len(value_counts) > max_categories:\n",
    "                print(f\"... and {len(value_counts) - max_categories} more categories\")\n",
    "                \n",
    "            results[col]['top_values'] = value_counts.head(top_n).to_dict()\n",
    "            \n",
    "            # Plot bar chart if requested\n",
    "            if plot_all:\n",
    "                plt.figure(figsize=figsize)\n",
    "                \n",
    "                if len(value_counts) > max_categories:\n",
    "                    # For too many categories, show top N and group others\n",
    "                    top_values = value_counts.head(max_categories-1)\n",
    "                    others_sum = value_counts.iloc[max_categories-1:].sum()\n",
    "                    \n",
    "                    combined = pd.concat([\n",
    "                        top_values, \n",
    "                        pd.Series({'Others': others_sum})\n",
    "                    ])\n",
    "                    \n",
    "                    combined.plot.barh()\n",
    "                    print(f\"Grouped {len(value_counts) - (max_categories-1)} categories as 'Others'\")\n",
    "                else:\n",
    "                    value_counts.plot.barh()\n",
    "                    \n",
    "                plt.ylabel(col)\n",
    "                plt.xlabel('count')\n",
    "                plt.title(f'Distribution of {col}')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    # Print summary of invisible data\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY OF NON-VISIBLE DATA:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if invisible_data:\n",
    "        for col, issues in invisible_data.items():\n",
    "            print(f\"\\n{col}:\")\n",
    "            for issue_type, count in issues.items():\n",
    "                print(f\"  - {issue_type}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"No non-visible data detected\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Basic usage\n",
    "results = analyze_and_visualize_dataframe(df)\n",
    "\n",
    "# Custom usage\n",
    "results = analyze_and_visualize_dataframe(\n",
    "    df,\n",
    "    figsize=(8, 4),\n",
    "    continuous_bins=30,\n",
    "    log_threshold=100,\n",
    "    max_categories=10,\n",
    "    exclude_cols=['id', 'customer_id']  # Skip these columns\n",
    ")\n",
    "\n",
    "# Just analyze without plotting\n",
    "analysis = analyze_and_visualize_dataframe(df, plot_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot\n",
    "\n",
    "| *Visualization types*        \t|    column continuous    \t| column categorical \t|\n",
    "|---------------------\t|:----------------------:\t|:-----------------:\t|\n",
    "| __column continuous__    \t| scatter plot, heatmap \t| category-specific histograms, box plot, violin plot |\n",
    "| __column categorical__   \t| category-specific histograms, box plot, violin plot |  stacked bar plot  \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scatter plot- points position distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "df.plot.scatter('age','hours-per-week',s=10,alpha=0.1) # alpha=0.1,s=10\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### heatmap - number of points distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_histogram(df, x_col, y_col, nbins=40, figsize=(5, 3), log_scale=True, \n",
    "                      min_count=0.1, tick_freq=4, cmap=None, title=None):\n",
    "    \"\"\"\n",
    "    Create a 2D histogram heatmap from two DataFrame columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing the data\n",
    "    x_col : str\n",
    "        Column name for x-axis\n",
    "    y_col : str\n",
    "        Column name for y-axis\n",
    "    nbins : int, optional (default=40)\n",
    "        Number of bins for both axes\n",
    "    figsize : tuple, optional (default=(5, 3))\n",
    "        Figure size (width, height) in inches\n",
    "    log_scale : bool, optional (default=True)\n",
    "        Whether to apply log10 to the counts\n",
    "    min_count : float, optional (default=0.1)\n",
    "        Minimum value to replace zeros with (for log scaling)\n",
    "    tick_freq : int, optional (default=4)\n",
    "        Frequency of tick marks on both axes\n",
    "    cmap : str or colormap, optional (default=None)\n",
    "        Matplotlib colormap to use\n",
    "    title : str, optional (default=None)\n",
    "        Title for the plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure object containing the plot\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create 2D histogram\n",
    "    heatmap, xedges, yedges = np.histogram2d(df[x_col], df[y_col], bins=nbins)\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    \n",
    "    # Replace zeros to avoid log(0) if using log scale\n",
    "    if log_scale:\n",
    "        heatmap[heatmap == 0] = min_count\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    if log_scale:\n",
    "        plt.imshow(np.log10(heatmap).T, origin='lower', vmin=0, extent=extent, cmap=cmap)\n",
    "        cbar_label = 'log10(count)'\n",
    "    else:\n",
    "        plt.imshow(heatmap.T, origin='lower', extent=extent, cmap=cmap)\n",
    "        cbar_label = 'count'\n",
    "    \n",
    "    # Add labels and ticks\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    \n",
    "    # Add ticks at specified frequency\n",
    "    x_ticks = np.linspace(xedges[0], xedges[-1], nbins+1)[::tick_freq]\n",
    "    y_ticks = np.linspace(yedges[0], yedges[-1], nbins+1)[::tick_freq]\n",
    "    plt.xticks(x_ticks, x_ticks.astype(int))\n",
    "    plt.yticks(y_ticks, y_ticks.astype(int))\n",
    "    \n",
    "    # Add colorbar and title\n",
    "    plt.colorbar(label=cbar_label)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Basic usage\n",
    "fig = plot_2d_histogram(df, 'age', 'hours-per-week')\n",
    "plt.show()\n",
    "\n",
    "# Customized usage\n",
    "fig = plot_2d_histogram(\n",
    "    df, \n",
    "    'age', \n",
    "    'hours-per-week', \n",
    "    nbins=50, \n",
    "    figsize=(8, 5), \n",
    "    log_scale=True, \n",
    "    cmap='viridis', \n",
    "    title='Age vs Hours per Week Distribution'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stacked bar plot - percentage distribution of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stacked_categorical_distribution(df, x_col, y_col, figsize=(5, 3), legend_loc=4, \n",
    "                                          title=None, normalize=True, rotation=0):\n",
    "    \"\"\"\n",
    "    Create a stacked bar chart showing the distribution of two categorical variables.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing the data\n",
    "    x_col : str\n",
    "        Column name for x-axis (categories to compare)\n",
    "    y_col : str\n",
    "        Column name for values to stack within each x category\n",
    "    figsize : tuple, optional (default=(5, 3))\n",
    "        Figure size (width, height) in inches\n",
    "    legend_loc : int or str, optional (default=4)\n",
    "        Location of the legend (1-10 or 'best', 'upper right', etc.)\n",
    "    title : str, optional (default=None)\n",
    "        Title for the plot\n",
    "    normalize : bool, optional (default=True)\n",
    "        Whether to normalize rows to show proportions instead of counts\n",
    "    rotation : int, optional (default=0)\n",
    "        Rotation angle for x-tick labels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure object containing the plot\n",
    "    pandas.DataFrame\n",
    "        The count or normalized matrix used for plotting\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Count occurrences of each combination\n",
    "    count_matrix = df.groupby([x_col, y_col]).size().unstack(fill_value=0)\n",
    "    \n",
    "    if normalize:\n",
    "        # Normalize rows to show proportions\n",
    "        matrix_for_plot = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "        ylabel = f'Fraction of {x_col} in each group'\n",
    "    else:\n",
    "        # Use raw counts\n",
    "        matrix_for_plot = count_matrix\n",
    "        ylabel = 'Count'\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    matrix_for_plot.plot(kind='bar', stacked=True)\n",
    "    \n",
    "    # Add labels and customize\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(x_col)\n",
    "    plt.legend(loc=legend_loc, title=y_col)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    # Set rotation for x-tick labels\n",
    "    plt.xticks(rotation=rotation)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, matrix_for_plot\n",
    "\n",
    "# Basic usage with default settings\n",
    "fig, matrix = plot_stacked_categorical_distribution(df, 'race', 'gross-income')\n",
    "plt.show()\n",
    "\n",
    "# Customized usage\n",
    "fig, matrix = plot_stacked_categorical_distribution(\n",
    "    df, \n",
    "    'race', \n",
    "    'gross-income', \n",
    "    figsize=(10, 6),\n",
    "    legend_loc='lower right',\n",
    "    title='Income Distribution by Race',\n",
    "    normalize=True,\n",
    "    rotation=45\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Access the matrix data\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### category-specific histograms - continuous distribution split by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_by_category(df, value_col, category_col, figsize=(8, 5), bins=20, \n",
    "                               alpha=0.5, density=True, range_tuple=None, \n",
    "                               title=None, colors=None, grid=True):\n",
    "    \"\"\"\n",
    "    Plot overlapping histograms of a value column for each category in a categorical column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing the data\n",
    "    value_col : str\n",
    "        Column name for the continuous variable to histogram (x-axis)\n",
    "    category_col : str\n",
    "        Column name for the categorical variable that defines groups\n",
    "    figsize : tuple, optional (default=(8, 5))\n",
    "        Figure size (width, height) in inches\n",
    "    bins : int, optional (default=20)\n",
    "        Number of bins for the histogram\n",
    "    alpha : float, optional (default=0.5)\n",
    "        Transparency level for the histogram bars\n",
    "    density : bool, optional (default=True)\n",
    "        Whether to normalize the histogram to show density instead of counts\n",
    "    range_tuple : tuple, optional (default=None)\n",
    "        Range of values to include in the histogram (min, max)\n",
    "        If None, the range is determined from the data\n",
    "    title : str, optional (default=None)\n",
    "        Title for the plot\n",
    "    colors : list or dict, optional (default=None)\n",
    "        List of colors for each category or dictionary mapping categories to colors\n",
    "    grid : bool, optional (default=True)\n",
    "        Whether to show grid lines\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure object containing the plot\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get unique categories\n",
    "    categories = df[category_col].unique()\n",
    "    \n",
    "    # Determine range if not provided\n",
    "    if range_tuple is None:\n",
    "        range_tuple = (df[value_col].min(), df[value_col].max())\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create color mapping if colors provided as list\n",
    "    if colors is not None and not isinstance(colors, dict):\n",
    "        colors = {cat: color for cat, color in zip(categories, colors)}\n",
    "    \n",
    "    # Plot histograms for each category\n",
    "    for i, category in enumerate(categories):\n",
    "        # Get data for this category\n",
    "        data = df[df[category_col] == category][value_col]\n",
    "        \n",
    "        # Skip if no data\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get color for this category\n",
    "        if colors is not None and isinstance(colors, dict):\n",
    "            color = colors.get(category)\n",
    "            plt.hist(data, alpha=alpha, label=category, range=range_tuple, \n",
    "                    bins=bins, density=density, color=color)\n",
    "        else:\n",
    "            plt.hist(data, alpha=alpha, label=category, range=range_tuple, \n",
    "                    bins=bins, density=density)\n",
    "    \n",
    "    # Add labels and customize\n",
    "    plt.xlabel(value_col)\n",
    "    plt.ylabel('Density' if density else 'Count')\n",
    "    plt.legend(title=category_col)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    if grid:\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Basic usage with default settings\n",
    "fig = plot_histograms_by_category(df, 'age', 'gross-income')\n",
    "plt.show()\n",
    "\n",
    "# Customized usage\n",
    "fig = plot_histograms_by_category(\n",
    "    df, \n",
    "    'age', \n",
    "    'gross-income', \n",
    "    figsize=(10, 6),\n",
    "    bins=30,\n",
    "    alpha=0.7,\n",
    "    density=True,\n",
    "    title='Age Distribution by Income Level',\n",
    "    colors={'<=50K': 'blue', '>50K': 'red'}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age','gross-income']].boxplot(by='gross-income')\n",
    "plt.ylabel('age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dataset = [df[df['gross-income']==' <=50K']['age'].values,\n",
    "           df[df['gross-income']==' >50K']['age'].values]\n",
    "\n",
    "plt.violinplot(dataset = dataset)\n",
    "plt.xticks([1,2],['<=50k','>50k'])\n",
    "plt.ylabel('age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correlation & autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Temp and lagged Temp:\n",
      "0.9276437214262888\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, Optional\n",
    "import seaborn as sns\n",
    "\n",
    "def correlation(x: pd.Series, y: pd.Series, method: str = 'pearson') -> float:\n",
    "    \"\"\"\n",
    "    Compute correlation between two series with automatic alignment\n",
    "    \n",
    "    Args:\n",
    "        x: First input series\n",
    "        y: Second input series\n",
    "        method: Correlation method ('pearson', 'spearman', 'kendall')\n",
    "        \n",
    "    Returns:\n",
    "        Correlation coefficient\n",
    "    \"\"\"\n",
    "    # Align and drop NA pairs\n",
    "    aligned = pd.concat([x, y], axis=1).dropna()\n",
    "    if len(aligned) < 2:\n",
    "        return np.nan\n",
    "    return aligned.iloc[:, 0].corr(aligned.iloc[:, 1], method=method)\n",
    "\n",
    "def feature_target_correlation(\n",
    "    X: Union[pd.DataFrame, np.ndarray],\n",
    "    y: Union[pd.Series, np.ndarray],\n",
    "    method: str = 'pearson',\n",
    "    top_n: Optional[int] = None,\n",
    "    visualize: bool = True,\n",
    "    figsize: tuple = (10, 6)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute correlation between all features and target variable.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (DataFrame or array)\n",
    "        y: Target variable (Series or array)\n",
    "        method: Correlation method ('pearson', 'spearman', 'kendall')\n",
    "        top_n: Show only top N most correlated features (None for all)\n",
    "        visualize: Whether to create a bar plot\n",
    "        figsize: Figure size when visualizing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with correlations and p-values (sorted by absolute correlation)\n",
    "    \"\"\"\n",
    "    # Convert inputs to pandas if they aren't already\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_results = []\n",
    "    for col in X.columns:\n",
    "        # Drop NA pairs for each feature-target combination\n",
    "        valid = pd.concat([X[col], y], axis=1).dropna()\n",
    "        if len(valid) < 2:\n",
    "            corr = np.nan\n",
    "            pval = np.nan\n",
    "        else:\n",
    "            corr = valid.iloc[:, 0].corr(valid.iloc[:, 1], method=method)\n",
    "            # Calculate p-value for pearson correlation\n",
    "            if method == 'pearson':\n",
    "                from scipy.stats import pearsonr\n",
    "                _, pval = pearsonr(valid.iloc[:, 0], valid.iloc[:, 1])\n",
    "            else:\n",
    "                pval = np.nan\n",
    "        \n",
    "        corr_results.append({\n",
    "            'feature': col,\n",
    "            'correlation': corr,\n",
    "            'abs_correlation': abs(corr),\n",
    "            'p_value': pval\n",
    "        })\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = pd.DataFrame(corr_results).sort_values('abs_correlation', ascending=False)\n",
    "    \n",
    "    # Filter top N if requested\n",
    "    if top_n is not None:\n",
    "        results = results.head(top_n)\n",
    "    \n",
    "    # Visualization\n",
    "    if visualize:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.barplot(\n",
    "            data=results,\n",
    "            x='correlation',\n",
    "            y='feature',\n",
    "            palette='coolwarm',\n",
    "            orient='h'\n",
    "        )\n",
    "        plt.axvline(0, color='black', linestyle='--')\n",
    "        plt.title(f'Feature-Target Correlations ({method.title()} Method)')\n",
    "        plt.xlabel('Correlation Coefficient')\n",
    "        plt.ylabel('Features')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create test data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2020-01-01', periods=365*3)\n",
    "    temp = 20 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365) + np.random.normal(0, 2, len(dates))\n",
    "    df_temp = pd.DataFrame({'Date': dates, 'Temp': temp}).set_index('Date')\n",
    "\n",
    "    # Test correlation method\n",
    "    print(\"Correlation between Temp and lagged Temp:\")\n",
    "    print(correlation(df_temp['Temp'], df_temp['Temp'].shift(1)))\n",
    "\n",
    "    # Compare with pandas built-in\n",
    "    pd.plotting.autocorrelation_plot(df_temp['Temp'])\n",
    "    plt.title(\"Pandas Built-in Autocorrelation\")\n",
    "    plt.show()\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'age': np.random.randint(20, 70, 100),\n",
    "        'income': np.random.normal(50000, 15000, 100),\n",
    "        'education': np.random.choice(['High School', 'College', 'Grad'], 100),\n",
    "        'credit_score': np.random.randint(300, 850, 100),\n",
    "        'target': np.random.normal(0, 1, 100) + 0.5 * np.random.randint(20, 70, 100)/100\n",
    "    })\n",
    "\n",
    "\n",
    "    # # One-hot encode categorical variables\n",
    "    # data = pd.get_dummies(df, columns=[\n",
    "    #     'workclass', 'education','marital-status', 'occupation', \n",
    "    #     'relationship', 'race', 'sex', 'native-country', 'gross-income'\n",
    "    # ])\n",
    "    # results = feature_target_correlation(\n",
    "    #     X=data.drop('age', axis=1),\n",
    "    #     y=data['age'],\n",
    "    #     method='pearson',\n",
    "    #     figsize=(10,18),\n",
    "    #     visualize=True\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f_regression & mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(X, y, problem_type='regression', k_best=None, percentile=None, visualize=True, random_state=10):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using F-test and mutual information, and select top features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas DataFrame or numpy.ndarray\n",
    "        Feature matrix\n",
    "    y : pandas Series or numpy.ndarray\n",
    "        Target variable\n",
    "    problem_type : str, optional (default='regression')\n",
    "        Type of problem: 'regression' or 'classification'\n",
    "    k_best : int, optional (default=None)\n",
    "        Number of top features to select using SelectKBest\n",
    "    percentile : int, optional (default=None)\n",
    "        Percentile of top features to select using SelectPercentile\n",
    "    visualize : bool, optional (default=True)\n",
    "        Whether to visualize feature relationships\n",
    "    random_state : int, optional (default=10)\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing f_test, p_values, mutual_information, selected_features_k_best,\n",
    "        selected_features_percentile, and the transformed X data\n",
    "        \n",
    "        f_test: Linear relationships between features and target (The larger the better)\n",
    "        p_values: array of p-values(The smaller the better, <0.5 means significant)\n",
    "        mutual_information: Any statistical dependency (linear or non-linear) (The larger the better)\n",
    "        selected_features_k_best: array of boolean values indicating selected features\n",
    "        selected_features_percentile: array of boolean values indicating selected features\n",
    "        X_k_best: array of selected features\n",
    "        X_percentile: array of selected features\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_selection import (\n",
    "        f_regression, mutual_info_regression, \n",
    "        f_classif, mutual_info_classif,\n",
    "        SelectKBest, SelectPercentile\n",
    "    )\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Handle pandas DataFrame/Series if provided\n",
    "    X_is_pandas = isinstance(X, pd.DataFrame)\n",
    "    feature_names = X.columns if X_is_pandas else None\n",
    "    \n",
    "    # Convert to numpy arrays for computation if needed\n",
    "    X_np = X.values if X_is_pandas else X\n",
    "    y_np = y.values if isinstance(y, pd.Series) else y\n",
    "    \n",
    "    # Select appropriate scoring functions based on problem type\n",
    "    if problem_type == 'regression':\n",
    "        f_test_func = f_regression\n",
    "        mi_func = mutual_info_regression\n",
    "    elif problem_type == 'classification':\n",
    "        f_test_func = f_classif\n",
    "        mi_func = mutual_info_classif\n",
    "    else:\n",
    "        raise ValueError(\"problem_type must be 'regression' or 'classification'\")\n",
    "    \n",
    "    # Calculate feature importance metrics\n",
    "    f_test, p_values = f_test_func(X_np, y_np)\n",
    "    mi = mi_func(X_np, y_np)\n",
    "    \n",
    "    # Print feature importance metrics with feature names if available\n",
    "    if X_is_pandas:\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'F-Score': f_test,\n",
    "            'P-Value': p_values,\n",
    "            'Mutual Info': mi\n",
    "        })\n",
    "        print(feature_importance)\n",
    "    else:\n",
    "        print('f score', f_test)\n",
    "        print('p values', p_values)\n",
    "        print('mi', mi)\n",
    "    \n",
    "    # Visualization\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i in range(min(3, X_np.shape[1])):  # Show at most 3 features\n",
    "            plt.subplot(1, min(3, X_np.shape[1]), i + 1)\n",
    "            plt.scatter(X_np[:, i], y_np, edgecolor='black', s=20)\n",
    "            \n",
    "            # Use feature names if available\n",
    "            if X_is_pandas:\n",
    "                plt.xlabel(f\"{feature_names[i]}\", fontsize=14)\n",
    "            else:\n",
    "                plt.xlabel(f\"$x_{{{i + 1}}}$\", fontsize=14)\n",
    "                \n",
    "            if i == 0:\n",
    "                plt.ylabel(\"$y$\", fontsize=14)\n",
    "            plt.title(f\"F-test={f_test[i]:.2f}, MI={mi[i]:.2f}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Initialize return values\n",
    "    result = {\n",
    "        'f_test': f_test,\n",
    "        'p_values': p_values,\n",
    "        'mutual_information': mi,\n",
    "        'selected_features_k_best': None,\n",
    "        'selected_features_percentile': None,\n",
    "        'X_k_best': None,\n",
    "        'X_percentile': None\n",
    "    }\n",
    "    \n",
    "    if k_best:\n",
    "        # Select k best features\n",
    "        f_select = SelectKBest(mi_func, k=k_best)\n",
    "        X_k_best = f_select.fit_transform(X_np, y_np)\n",
    "        selected_features_k_best = f_select.get_support()\n",
    "        \n",
    "        if visualize and k_best == 1:  # Only show if k=1\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.scatter(X_k_best, y_np, edgecolor='k')\n",
    "            plt.xlabel('Selected Feature')\n",
    "            plt.ylabel('Target')\n",
    "            plt.title('Relationship with Selected Feature')\n",
    "            plt.show()\n",
    "        \n",
    "        # Return a DataFrame if input was DataFrame\n",
    "        if X_is_pandas:\n",
    "            selected_feature_names = feature_names[selected_features_k_best]\n",
    "            print(\"Selected features (k_best):\", selected_feature_names.tolist())\n",
    "            X_k_best = pd.DataFrame(X_k_best, columns=selected_feature_names, index=X.index)\n",
    "        else:\n",
    "            print(\"Selected features (k_best):\", selected_features_k_best)\n",
    "        \n",
    "        result['selected_features_k_best'] = selected_features_k_best\n",
    "        result['X_k_best'] = X_k_best\n",
    "    \n",
    "    if percentile:  \n",
    "        # Select features by percentile\n",
    "        f_selector = SelectPercentile(mi_func, percentile=percentile)\n",
    "        X_percentile = f_selector.fit_transform(X_np, y_np)\n",
    "        selected_features_percentile = f_selector.get_support()\n",
    "        \n",
    "        # Return a DataFrame if input was DataFrame\n",
    "        if X_is_pandas:\n",
    "            selected_feature_names = feature_names[selected_features_percentile]\n",
    "            print(\"Selected features (percentile):\", selected_feature_names.tolist())\n",
    "            X_percentile = pd.DataFrame(X_percentile, columns=selected_feature_names, index=X.index)\n",
    "        else:\n",
    "            print(\"Selected features (percentile):\", selected_features_percentile)\n",
    "            \n",
    "        result['selected_features_percentile'] = selected_features_percentile\n",
    "        result['X_percentile'] = X_percentile\n",
    "    \n",
    "    return result\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(10)\n",
    "X_np = np.random.rand(1000, 3)\n",
    "y_np = X_np[:, 0] + np.sin(6 * np.pi * X_np[:, 1]) + 0.1 * X_np[:, 2]\n",
    "\n",
    "# Convert to pandas DataFrame/Series\n",
    "X = pd.DataFrame(X_np, columns=['feature_1', 'feature_2', 'feature_3'])\n",
    "y = pd.Series(y_np, name='target')\n",
    "\n",
    "# Analyze feature importance\n",
    "results = analyze_feature_importance(X, y, k_best=2, percentile=66)\n",
    "\n",
    "# Access results\n",
    "if results['selected_features_k_best'] is not None:\n",
    "    print(\"\\nTop 2 features:\", X.columns[results['selected_features_k_best']].tolist())\n",
    "if results['selected_features_percentile'] is not None:\n",
    "    print(\"Features in top 66 percentile:\", X.columns[results['selected_features_percentile']].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "# create a test set without considering groups\n",
    "X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state=0,stratify=y)\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "# create a test set based on groups\n",
    "groups = df['patient ID'] # a categorical column\n",
    "splitter = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=0)\n",
    "for i_other,i_test in splitter.split(X, y, groups):\n",
    "    X_other, y_other, groups_other = X.iloc[i_other], y.iloc[i_other], groups.iloc[i_other]\n",
    "    X_test, y_test, groups_test = X.iloc[i_test], y.iloc[i_test], groups.iloc[i_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cv search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kf = GroupKFold(n_splits=10)\n",
    "# create the pipeline: preprocessor + supervised ML method\n",
    "scaler = StandardScaler()\n",
    "pipe = make_pipeline(scaler,SVC())\n",
    "# the parameter(s) we want to tune\n",
    "param_grid = {'svc__C': np.logspace(-3,4,num=8),'svc__gamma': np.logspace(-3,4,num=8)}\n",
    "# prepare gridsearch\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid,scoring = make_scorer(accuracy_score),\n",
    "                    cv=kf, return_train_score = True)\n",
    "# do kfold CV on _other\n",
    "grid.fit(X_other, y_other, groups=groups_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. handle missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_missing_value(df, plot=True):\n",
    "    print('data dimensions:',df.shape)\n",
    "    perc_missing_per_ftr = df.isnull().sum(axis=0)/df.shape[0]\n",
    "    print('fraction of missing values in features:')\n",
    "    print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "    print('data types of the features with missing values:')\n",
    "    print(df[perc_missing_per_ftr[perc_missing_per_ftr > 0].index].dtypes)\n",
    "    frac_missing = sum(df.isnull().sum(axis=1)!=0)/df.shape[0]\n",
    "    print('fraction of points with missing values:',frac_missing)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plot_df = perc_missing_per_ftr[perc_missing_per_ftr > 0].sort_values(ascending=False)\n",
    "        plot_df.plot(kind='bar', color='steelblue')\n",
    "\n",
    "        plt.xlabel('feature')\n",
    "        plt.ylabel('fraction of missing values')\n",
    "        plt.title('Fraction of missing value of each feature')\n",
    "\n",
    "        plt.ylim(0, max(perc_missing_per_ftr) * 1.15)\n",
    "\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "visualize_missing_value(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop missing value by row/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "# by default, rows/points are dropped\n",
    "df_r = df.dropna()\n",
    "print(df_r.shape)\n",
    "# drop features with missing values\n",
    "df_c = df.dropna(axis=1)\n",
    "print(df_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('missing')\n",
    "\n",
    "df['A'] = df['A'].fillna(0)\n",
    "df['C'] = df['C'].fillna(df['C'].mean())\n",
    "\n",
    "df.fillna({'A': -1, 'B': 'empty'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### onehot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_encoded = pd.get_dummies(df, columns=['column1', 'column2']) # , drop_first=True\n",
    "\n",
    "# Align test data with same columns\n",
    "test_df = df\n",
    "test_encoded = pd.get_dummies(test_df, columns=['color'])\n",
    "test_encoded = test_encoded.reindex(columns=train_encoded.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ordinal_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_encode_categories(df, columns, category_orders=None, suffix='_encoded', \n",
    "                             inplace=False, return_encoder=False):\n",
    "    \"\"\"\n",
    "    Encode categorical columns using OrdinalEncoder with specified or inferred category orders.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing the categorical columns to encode\n",
    "    columns : list\n",
    "        List of column names to encode\n",
    "    category_orders : dict or None, optional (default=None)\n",
    "        Dictionary mapping column names to their desired category orders\n",
    "        Example: {'size': ['small', 'medium', 'large']}\n",
    "        If None, categories will be determined from the data\n",
    "    suffix : str, optional (default='_encoded')\n",
    "        Suffix to add to column names for the encoded versions\n",
    "    inplace : bool, optional (default=False)\n",
    "        Whether to modify the original DataFrame or create a copy\n",
    "    return_encoder : bool, optional (default=False)\n",
    "        Whether to return the fitted encoder object\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with added encoded columns\n",
    "    sklearn.preprocessing.OrdinalEncoder (optional)\n",
    "        The fitted encoder object, returned only if return_encoder=True\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Make a copy if not inplace\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    # Prepare categories\n",
    "    if category_orders is None:\n",
    "        # If no orders provided, let the encoder determine categories from the data\n",
    "        encoder = OrdinalEncoder()\n",
    "    else:\n",
    "        # Use provided category orders\n",
    "        categories = [category_orders.get(col, None) for col in columns]\n",
    "        # For any None values, determine from data\n",
    "        for i, cats in enumerate(categories):\n",
    "            if cats is None:\n",
    "                # Replace None with unique values from the column\n",
    "                categories[i] = sorted(df[columns[i]].unique())\n",
    "        encoder = OrdinalEncoder(categories=categories)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    encoded_data = encoder.fit_transform(df[columns])\n",
    "    \n",
    "    # Create column names for encoded data\n",
    "    encoded_columns = [f\"{col}{suffix}\" for col in columns]\n",
    "    \n",
    "    # Convert to DataFrame and add to original\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=encoded_columns, index=df.index)\n",
    "    \n",
    "    # Add encoded columns to the original DataFrame\n",
    "    for col in encoded_columns:\n",
    "        df[col] = df_encoded[col]\n",
    "    \n",
    "    # Return results\n",
    "    if return_encoder:\n",
    "        return df, encoder\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "# Apply ordinal encoding\n",
    "df_encoded = ordinal_encode_categories(df, ['education', 'workclass'])\n",
    "print(df_encoded.columns)\n",
    "print(df_encoded[['education_encoded', 'workclass_encoded']])\n",
    "\n",
    "# If you need the encoder for later use:\n",
    "df_encoded, encoder = ordinal_encode_categories(df, ['education', 'workclass'], return_encoder=True)\n",
    "\n",
    "# To transform new data with the same encoder:\n",
    "# new_encoded_data = encoder.transform(new_df[['size', 'priority']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical_df(df, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Standardize numerical features in a DataFrame (z-score normalization)\n",
    "    Args:\n",
    "        df: Input pandas DataFrame\n",
    "        mean: Precomputed mean Series (if None, computes from df)\n",
    "        std: Precomputed std Series (if None, computes from df)\n",
    "    Returns:\n",
    "        Scaled DataFrame and computed (mean, std)\n",
    "    \"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    if mean is None or std is None:\n",
    "        # Compute mean and std while ignoring NaN values\n",
    "        mean = df[numerical_cols].mean(skipna=True)\n",
    "        std = df[numerical_cols].std(skipna=True)\n",
    "        std[std == 0] = 1.0  # avoid division by zero\n",
    "    \n",
    "    # Create copy to avoid SettingWithCopyWarning\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Fill NaN with mean and apply scaling\n",
    "    for col in numerical_cols:\n",
    "        df_scaled[col] = df[col].fillna(mean[col])  # Replace NaN with mean\n",
    "        df_scaled[col] = (df_scaled[col] - mean[col]) / std[col]\n",
    "    \n",
    "    return df_scaled, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale_df(df, min_val=None, max_val=None, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Min-Max scaling for numerical features in a DataFrame\n",
    "    Args:\n",
    "        df: Input pandas DataFrame\n",
    "        min_val: Precomputed min Series (if None, computes from df)\n",
    "        max_val: Precomputed max Series (if None, computes from df)\n",
    "        feature_range: Desired range of transformed data (default 0-1)\n",
    "    Returns:\n",
    "        Scaled DataFrame and computed (min_val, max_val)\n",
    "    \"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    if min_val is None or max_val is None:\n",
    "        # Compute min and max while ignoring NaN values\n",
    "        min_val = df[numerical_cols].min(skipna=True)\n",
    "        max_val = df[numerical_cols].max(skipna=True)\n",
    "    \n",
    "    # Create copy to avoid SettingWithCopyWarning\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Handle constant features and calculate ranges\n",
    "    ranges = max_val - min_val\n",
    "    ranges[ranges == 0] = 1.0  # avoid division by zero\n",
    "    \n",
    "    a, b = feature_range\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        # Replace NaN with midpoint of min/max\n",
    "        df_scaled[col] = df[col].fillna(min_val[col] + ranges[col]/2)\n",
    "        \n",
    "        # Scale to [0, 1] then to target range\n",
    "        df_scaled[col] = ((df_scaled[col] - min_val[col]) / ranges[col]) * (b - a) + a\n",
    "    \n",
    "    return df_scaled, min_val, max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_imputer_df(df, strategy='mean', fill_value=0):\n",
    "    \"\"\"\n",
    "    Basic missing value imputation for DataFrames\n",
    "    Args:\n",
    "        df: Input pandas DataFrame\n",
    "        strategy: 'mean', 'median', 'constant', or 'mode'\n",
    "        fill_value: Value to use for 'constant' strategy\n",
    "    Returns:\n",
    "        Imputed DataFrame\n",
    "    \"\"\"\n",
    "    # Create copy to avoid SettingWithCopyWarning\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Handle numerical and categorical columns differently\n",
    "    num_cols = df.select_dtypes(include=['number']).columns\n",
    "    cat_cols = df.select_dtypes(exclude=['number']).columns\n",
    "    \n",
    "    # Numerical columns\n",
    "    for col in num_cols:\n",
    "        if strategy == 'mean':\n",
    "            fill_val = df[col].mean(skipna=True)\n",
    "        elif strategy == 'median':\n",
    "            fill_val = df[col].median(skipna=True)\n",
    "        elif strategy == 'mode':\n",
    "            fill_val = df[col].mode()[0]  # Take first mode if multiple\n",
    "        elif strategy == 'constant':\n",
    "            fill_val = fill_value\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "            \n",
    "        df_imputed[col] = df[col].fillna(fill_val)\n",
    "    \n",
    "    # Categorical columns (always use mode or constant)\n",
    "    for col in cat_cols:\n",
    "        if strategy in ['mean', 'median']:\n",
    "            # Default to mode for categorical when mean/median requested\n",
    "            fill_val = df[col].mode()[0]\n",
    "        elif strategy == 'mode':\n",
    "            fill_val = df[col].mode()[0]\n",
    "        else:  # constant\n",
    "            fill_val = fill_value\n",
    "            \n",
    "        df_imputed[col] = df[col].fillna(fill_val)\n",
    "    \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PolynomialRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PolynomialRegression(nn.Module):\n",
    "    def __init__(self, degree=2):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.weights = nn.Parameter(torch.randn(degree + 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate polynomial terms: [x^0, x^1, x^2, ..., x^degree]\n",
    "        powers = torch.stack([x**i for i in range(self.degree + 1)], dim=1)\n",
    "        return torch.matmul(powers, self.weights)\n",
    "\n",
    "# Example usage:\n",
    "def train_poly_model(X, y, degree=2, epochs=1000, lr=0.01):\n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    model = PolynomialRegression(degree=degree)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "X = torch.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X + 3 + torch.randn(X.size()) * 2  # y = 2x + 3 + noise\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Define the linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LinearRegression()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    test_loss = criterion(y_pred, y_test)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    \n",
    "    # Get the learned parameters\n",
    "    w = model.linear.weight.item()\n",
    "    b = model.linear.bias.item()\n",
    "    print(f'Learned parameters: w = {w:.2f}, b = {b:.2f}')\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_test.numpy(), y_test.numpy(), color='blue', label='Actual')\n",
    "plt.plot(X_test.numpy(), y_pred.numpy(), color='red', linewidth=2, label='Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression with PyTorch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate sample binary classification data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, \n",
    "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Define the logistic regression model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LogisticRegression(X.shape[1])\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred_class = (y_pred > 0.5).float()\n",
    "    accuracy = (y_pred_class == y_test).float().mean()\n",
    "    print(f'Test Accuracy: {accuracy.item()*100:.2f}%')\n",
    "    \n",
    "    # Get the learned parameters\n",
    "    w = model.linear.weight.detach().numpy().flatten()\n",
    "    b = model.linear.bias.item()\n",
    "    print(f'Learned parameters: w = {w}, b = {b:.2f}')\n",
    "\n",
    "# Plot decision boundary (for 2D data)\n",
    "if X.shape[1] == 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.numpy().flatten(), cmap='coolwarm', alpha=0.6)\n",
    "    \n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), \n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Predict probabilities\n",
    "    with torch.no_grad():\n",
    "        Z = model(torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contour(xx, yy, Z.numpy(), levels=[0.5], colors='black')\n",
    "    plt.title('Logistic Regression Decision Boundary')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create synthetic data for demonstration\n",
    "def create_test_data(n_samples=1000, n_features=10, random_state=42):\n",
    "    # Generate synthetic regression data\n",
    "    X, y = make_regression(n_samples=n_samples, n_features=n_features, \n",
    "                           random_state=random_state)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])\n",
    "    y = pd.Series(y, name='target')\n",
    "    \n",
    "    # Add some missing values randomly\n",
    "    mask = np.random.random(df.shape) < 0.05  # 5% of values will be missing\n",
    "    df[mask] = np.nan\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=random_state)\n",
    "    X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.25, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_cv, X_test, y_train, y_cv, y_test\n",
    "\n",
    "def train_and_evaluate_xgboost(X_train, X_cv, X_test, y_train, y_cv, y_test):\n",
    "    # Create and configure model\n",
    "    # max_depth |\tMaximum depth of a tree\t3 - 10\n",
    "    # min_child_weight |\tMinimum sum of instance weight (hessian) in a child(Higher -> stable)\t1 - 10\n",
    "    # gamma |\tMinimum loss reduction to make a split(Higher -> stable)\t0 - 5\n",
    "    # subsample |\tRow sampling ratio for each tree\t0.5 - 1.0\n",
    "    # colsample_bytree |\tFeature sampling ratio per tree\t0.5 - 1.0\n",
    "\n",
    "    # learning_rate |\tStep size shrinkage\t0.01 - 0.3\n",
    "    # n_estimators |\tNumber of boosting rounds (trees)\t100 - 1000+\n",
    "    # early_stopping_rounds |\tStop if no improvement after N rounds (needs validation set)\t10 - 100\n",
    "\n",
    "    # lambda |\tL2 regularization on leaf weights\t0 - 10\n",
    "    # alpha |\tL1 regularization on leaf weights\t0 - 10\n",
    "    # Parameter grid\n",
    "    \n",
    "    model = xgboost.XGBRegressor(\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1000,\n",
    "        random_state=0,\n",
    "        colsample_bytree=0.9,\n",
    "        subsample=0.66\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_cv_pred = model.predict(X_cv)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cv_rmse = np.sqrt(mean_squared_error(y_cv, y_cv_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print('CV RMSE:', cv_rmse)\n",
    "    print('Test RMSE:', test_rmse)\n",
    "    print('Test R2:', test_r2)\n",
    "    \n",
    "    return model, y_test_pred\n",
    "\n",
    "# Run the example\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate test data\n",
    "    X_train, X_cv, X_test, y_train, y_cv, y_test = create_test_data()\n",
    "    \n",
    "    print(\"Data shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_cv: {X_cv.shape}, y_cv: {y_cv.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    print(\"\\nMissing values in X_train:\", X_train.isna().sum().sum())\n",
    "    \n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    model, predictions = train_and_evaluate_xgboost(\n",
    "        X_train, X_cv, X_test, y_train, y_cv, y_test\n",
    "    )\n",
    "    \n",
    "    # Display feature importances\n",
    "    importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    print(\"\\nTop 5 feature importances:\")\n",
    "    print(importances.sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values_valid = explainer(X_test)\n",
    "shap.summary_plot(shap_values_valid, X_test)\n",
    "shap.plots.bar(shap_values_valid)\n",
    "\n",
    "shap_values_train = explainer(X_train)\n",
    "shap_values_valid = explainer(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values_train, X_train, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values_valid, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# - x axis: false positive rate (fpr = FP / (FP + TN))\n",
    "# - y axis: true positive rate (R = TP / (TP + FN))\n",
    "# - AUC = 1 is a perfect classifier\n",
    "# - AUC > 0.5 is above chance-level predictor\n",
    "# - AUC = 0.5 is a chance-level classifier\n",
    "# - AUC < 0.5 is a bad predictor\n",
    "# - AUC = 0 classifies all points incorrectly\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "# # the roc_curve function performs the same calculation\n",
    "fpr,tpr,p_crits = roc_curve(y_test,y_pred)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "print(roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P-R cure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score # the AUC of the P-R curve\n",
    "\n",
    "p,r,p_crits = precision_recall_curve(y_test,y_pred)\n",
    "\n",
    "print(average_precision_score(y_test,y_pred))\n",
    "\n",
    "plt.plot(p,r)\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.title('P-R curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_pred_np = y_pred.numpy().squeeze()\n",
    "y_pred_for_matrix = np.zeros(len(y_test))\n",
    "\n",
    "# Perform thresholding\n",
    "y_pred_for_matrix[y_pred_np > 0.5] = 1\n",
    "C = confusion_matrix(y_test,y_pred_for_matrix)\n",
    "disp = ConfusionMatrixDisplay(C,display_labels=['class 0', 'class 1'])\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### acc, recall, precision, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\beta < 1$, more weight to precision.\n",
    "\n",
    "If $\\beta > 1$, more weight to recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, fbeta_score\n",
    "print('accuracy',accuracy_score(y_test,y_pred))\n",
    "print('recall',recall_score(y_test,y_pred))\n",
    "print('precision',precision_score(y_test,y_pred))\n",
    "print('f1',fbeta_score(y_test,y_pred,beta=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "# Dummy dataset generator (for illustration)\n",
    "def generate_dummy_data(num_samples=1000, input_size=20, num_classes=2):\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    y = torch.randint(0, num_classes, (num_samples,))\n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "# Example model class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size=20, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Train + eval function\n",
    "def train_and_eval(dataset, model, optimizer, loss_fn, batch_size):\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Train for a few epochs\n",
    "    model.train()\n",
    "    for epoch in range(5):  # Keep it short for tuning\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            predicted = preds.argmax(dim=1)\n",
    "            correct += (predicted == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_grid_search(param_grid, train_fn, dataset, verbose=True):\n",
    "    \"\"\"\n",
    "    Manual Grid Search implementation, suitable for any training function, \n",
    "    such as PyTorch or scikit-learn models.\n",
    "\n",
    "    Args:\n",
    "        param_grid: list of dicts, where each dict is a combination of hyperparameters\n",
    "        train_fn: a function that takes one parameter set and returns (model_object, validation_score)\n",
    "        \n",
    "        verbose: whether to print progress during the search\n",
    "\n",
    "    Returns:\n",
    "        best_params: the parameter set with the highest score\n",
    "        best_model: the model corresponding to the best parameter set\n",
    "        all_results: list of all results (each with params and score)\n",
    "    \"\"\"\n",
    "    best_score = -float('inf')\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    all_results = []\n",
    "\n",
    "    for i, params in enumerate(param_grid):\n",
    "        model, score = train_fn(params, dataset)\n",
    "        all_results.append({'params': params, 'score': score})\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{i+1}/{len(param_grid)}] Params: {params} -> Score: {score:.4f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_model, all_results\n",
    "\n",
    "def train_model(params, dataset):\n",
    "    # Example: training a PyTorch model\n",
    "    lr = params['lr']\n",
    "    batch_size = params['batch_size']\n",
    "    hidden = params['hidden']\n",
    "    \n",
    "    # Initialize your model, optimizer, and loss function here\n",
    "    model = MyModel(hidden_size=hidden)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Pseudo-code: load data, train for several epochs, evaluate on validation set\n",
    "    val_accuracy = train_and_eval(dataset, model, optimizer, loss_fn, batch_size)\n",
    "    \n",
    "    return model, val_accuracy\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Construct the parameter grid (like in sklearn's GridSearch)\n",
    "param_grid = [\n",
    "    {'lr': lr, 'batch_size': bs, 'hidden': h}\n",
    "    for lr in [0.001, 0.01]\n",
    "    for bs in [32, 64]\n",
    "    for h in [128, 256]\n",
    "]\n",
    "\n",
    "dataset = generate_dummy_data()\n",
    "best_params, best_model, all_results = custom_grid_search(param_grid, train_model, dataset)\n",
    "print(\"âœ… Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Coef for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coef from logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Show coefficients and intercepts\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercepts:\", model.intercept_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
